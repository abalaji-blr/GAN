{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Aging_CycleGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AlM3-vMJWgZ",
        "colab_type": "text"
      },
      "source": [
        "# CycleGAN for Faces Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu_MyFNKJdjJ",
        "colab_type": "text"
      },
      "source": [
        "**Objective:** Implement CycleGAN for faces dataset.\n",
        "\n",
        "Dataset Location:  https://susanqq.github.io/UTKFace/\n",
        "\n",
        "Original CycleGAN Implementation in PyTorch is available at https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVRVP-38JkAD",
        "colab_type": "text"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn0QyoSGJGOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "897e15ff-9834-4b08-820d-0ad2741b8deb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilrHxlgxJtzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/App/CycleGAN/Faces/young.npz' /content\n",
        "!cp '/content/drive/My Drive/App/CycleGAN/Faces/old.npz'   /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSv6r4IJKXSm",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-W73nRQKZLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqU_dQuzKIlE",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOENxAiSKKjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3627b4f-0780-42ca-bfdd-ebe6f660e423"
      },
      "source": [
        "# load the compressed data\n",
        "young = np.load('/content/young.npz')\n",
        "young_images = young['arr_0']\n",
        "print(young_images.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1056, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bIJ0XHpKy17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f116a33-2327-4436-b3d4-7cc4364822b4"
      },
      "source": [
        "# load the old faces data\n",
        "old = np.load('/content/old.npz')\n",
        "old_images = old['arr_0']\n",
        "print(old_images.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1056, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ARU2V0wtekP",
        "colab_type": "text"
      },
      "source": [
        "## Rescale the values to -1 to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhFuyCjFting",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rescale_input(data_arr):\n",
        "  res_arr = (data_arr - 127.5) / 127.5\n",
        "  return res_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4zfd9-4M-T4",
        "colab_type": "text"
      },
      "source": [
        "## Install external packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfaQtk9KNBXX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "7e528c60-c1db-4da5-c204-831c16ea8a30"
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-qjgea1ao\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-qjgea1ao\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.16.4)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=139eb35b977bedd695b79fc2e4e3fe666792f65a19b6d838ba4daa05844d145a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-71_309hw/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdNRuss-McWZ",
        "colab_type": "text"
      },
      "source": [
        "## Build CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE8vSu-YM4Wv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8ed7792-f387-4586-b88f-1d5d992c1ded"
      },
      "source": [
        "# keras layers\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Lambda, add\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# from keras_contrib\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XH1pudsMege",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCV1LGMWMtAi",
        "colab_type": "text"
      },
      "source": [
        "For Discriminator:\n",
        "*  Use PatchGAN - only penalizes the structure at the scale of patches.\n",
        "* PatchGAN classifies the NxN patch is real or fake\n",
        "*  They have fewer parameters than the full image discriminator\n",
        "* PatchGAN are used in [Image to Image translation](https://arxiv.org/pdf/1611.07004.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMXEatLlK9gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator layer has the following\n",
        "#  * Conv2D - filter size: 4x4, strides:2\n",
        "#  * LeakyReLU\n",
        "#  * InstanceNormalization\n",
        "#\n",
        "def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
        "  d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  if normalization:\n",
        "      d = InstanceNormalization()(d)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM4jUnfmPLOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build discriminator uses PatchGAN\n",
        "# Uses the patch to classify the image is fake or real.\n",
        "# PatchGAN uses \n",
        "#  * kernel size 4x4\n",
        "#  * num filters double at each stage\n",
        "def build_discriminator(image_shape, num_start_filters=64):\n",
        "  img = Input(image_shape)\n",
        "  \n",
        "  d1 = d_layer(img, num_start_filters, normalization=False)\n",
        "  d2 = d_layer(d1, num_start_filters*2)\n",
        "  d3 = d_layer(d2, num_start_filters*4)\n",
        "  d4 = d_layer(d3, num_start_filters*8)\n",
        "\n",
        "  validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "  \n",
        "  model = Model(img, validity)\n",
        "  \n",
        "  # compile model\n",
        "  model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLJTv5VaPcjN",
        "colab_type": "text"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUTin9NrPec6",
        "colab_type": "text"
      },
      "source": [
        "Generator can be one of the following two things:\n",
        "\n",
        "     * Encoder : Decoder combo (UNet - uses skip connections)\n",
        "     or\n",
        "     * Encoder : Transformer : Decoder (Uses Residual blocks)\n",
        "     \n",
        " The Encoder shrinks the input image. Uses Conv layers (with strides:2).\n",
        " \n",
        " The Transformer uses residual blocks\n",
        " \n",
        " The Decoder expands the image with transpose Conv.\n",
        " \n",
        " Note: each layer will use LeakyReLU and InstanceNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC7dmwpmRxN3",
        "colab_type": "text"
      },
      "source": [
        "#### Resnet block\n",
        "\n",
        "Original paper uses **reflection padding**. Let's use **same** padding for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1hpz_QRPdn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "def resnet_block(r_i, layer_output, ks=3, s=1):\n",
        "    r = Lambda(lambda x: tf.pad(x, [[0,0],[1,1],[1,1],[0,0]],'REFLECT'))(r_i)\n",
        "    #r = ReflectionPadding2D(padding=(1,1))(r_i)\n",
        "    r = conv2d(r,layer_output,ks,s,padding= 'VALID')\n",
        "    r = InstanceNormalization()(r)\n",
        "    \n",
        "    r = Lambda(lambda x: tf.pad(x, [[0,0],[1,1],[1,1],[0,0]],'REFLECT'))(r)\n",
        "    #r = ReflectionPadding2D(padding=(1,1))(r)\n",
        "    r = conv2d(r,layer_output,ks,s,padding= 'VALID')\n",
        "    r = InstanceNormalization()(r)\n",
        "    \n",
        "    return add([r_i , r])\n",
        "'''\n",
        "  \n",
        "def resnet_block(n_filters, input_layer):\n",
        "  # first layer convolutional layer\n",
        "\tg = Conv2D(n_filters, (3,3), padding='same')(input_layer)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tg = Activation('relu')(g)\n",
        "  \n",
        "\t# second convolutional layer\n",
        "\tg = Conv2D(n_filters, (3,3), padding='same')(g)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "  \n",
        "\t# concatenate merge channel-wise with input layer\n",
        "\tg = Concatenate()([g, input_layer])\n",
        "\treturn g\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VWFSSSYhN4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define  generator model\n",
        "def build_generator(image_shape, n_resnet=9):\n",
        "\t \n",
        "\t# image input\n",
        "\tin_image = Input(shape=image_shape)\n",
        "  \n",
        "  ## \n",
        "\t# c7s1-64\n",
        "\tg = Conv2D(64, (7,7), padding='same')(in_image)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tg = Activation('relu')(g)\n",
        "  \n",
        "  ## down sample\n",
        "\t# d128\n",
        "\tg = Conv2D(128, (3,3), strides=(2,2), padding='same')(g)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tg = Activation('relu')(g)\n",
        "\t# d256\n",
        "\tg = Conv2D(256, (3,3), strides=(2,2), padding='same')(g)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tg = Activation('relu')(g)\n",
        "  \n",
        "\t# R256 - resnet blocks\n",
        "\tfor _ in range(n_resnet):\n",
        "\t\tg = resnet_block(256, g)\n",
        "    \n",
        "  ## upsample\n",
        "\t# u128\n",
        "\tg = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same')(g)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tg = Activation('relu')(g)\n",
        "\t# u64\n",
        "\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same')(g)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tg = Activation('relu')(g)\n",
        "\t# c7s1-3\n",
        "\tg = Conv2D(3, (7,7), padding='same')(g)\n",
        "\tg = InstanceNormalization(axis=-1)(g)\n",
        "\tout_image = Activation('tanh')(g)\n",
        "  \n",
        "\t# define model\n",
        "\tmodel = Model(in_image, out_image)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6G6BjFAlc0X",
        "colab_type": "text"
      },
      "source": [
        "### Build combined model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6eudZmpp2-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## build combined model generator1, discrimator1 and generator2\n",
        "##\n",
        "## 1. The cycle loss is given more weightage 10 time more than\n",
        "##    the adversarial loss.\n",
        "##\n",
        "## 2. The identity loss is half the weightage of the cycle loss\n",
        "##    so, it is 5\n",
        "\n",
        "def build_combined_model(image_shape, g_model1, d_model, g_model2):\n",
        "  # update the trainable flag\n",
        "  g_model1.trainable = True\n",
        "  d_model.trainable = False\n",
        "  g_model2.trainable = False\n",
        "  \n",
        "  # discriminator elelemnt\n",
        "  input_gen = Input(shape=image_shape)\n",
        "  \n",
        "  gen1_out = g_model1(input_gen)\n",
        "  output_d = d_model(gen1_out)\n",
        "  \n",
        "  # identity element\n",
        "  input_id = Input(shape=image_shape)\n",
        "  output_id = g_model1(input_id)\n",
        "  \n",
        "  # forward cycle\n",
        "  output_f = g_model2(gen1_out)\n",
        "  \n",
        "  # backward cycle\n",
        "  gen2_out = g_model2(input_id)\n",
        "  output_b = g_model1(gen2_out)\n",
        "  \n",
        "  #define combined model\n",
        "  model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
        "  \n",
        "  #define the optimizer\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  \n",
        "  # compile model with weighting of least squares loss and L1 loss\n",
        "  ## Cycle loss is 10 times more than adv. loss\n",
        "  ## ident loss is half the cylce loss\n",
        "  model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgIjj1Lv8-Eu",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrq3PrUz2gLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate real samples\n",
        "def gen_real_samples(dataset, n_samples, patch_size):\n",
        "  # get random image\n",
        "  idx = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  \n",
        "  X = dataset[idx]\n",
        "  \n",
        "  # generate labels as well.\n",
        "  y = np.ones((n_samples, patch_size, patch_size, 1))\n",
        "  return X, y\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tuhUpAK9_Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate fake images from generator\n",
        "def gen_fake_samples(g_model, images, patch_size):\n",
        "  # use model predictor\n",
        "  fake_images = g_model.predict(images)\n",
        "  \n",
        "  # mark the labels as zero for fake.\n",
        "  y = np.zeros((len(fake_images), patch_size, patch_size, 1))\n",
        "  return fake_images, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SiPqD9H_Jj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save models to file\n",
        "def save_models(epoch, directory, gen_AB, gen_BA):\n",
        "  file_name1 = 'gen_modelAB_e%03d.hdf5' % (epoch)\n",
        "  path1 = os.path.join(directory, file_name1)\n",
        "  \n",
        "  file_name2 = 'gen_modelBA_e%03d.hdf5' % (epoch)\n",
        "  path2 = os.path.join(directory, file_name2)\n",
        "  \n",
        "  gen_AB.save(path1)\n",
        "  gen_BA.save(path2)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0542p_YBE4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample images at regular intervals to see the progress\n",
        "def sample_images(epoch, step, gen_model, trainX, prefix, directory, n_samples=5):\n",
        "  # get the real samples\n",
        "  X_in, _ = gen_real_samples(trainX, n_samples, 0)\n",
        "  \n",
        "  # get the generated / translated images\n",
        "  X_out, _ = gen_fake_samples(gen_model, X_in, 0)\n",
        "  \n",
        "  ## rescale [-1,1] to [0,1]\n",
        "  X_in = (X_in + 1) / 2.0\n",
        "  X_out = (X_out + 1) / 2.0\n",
        "  \n",
        "  ## generate the plot fig to save.\n",
        "  ## plt.subplot(nrows, ncols, index)\n",
        "  n_rows = 2\n",
        "  n_cols = n_samples\n",
        "  \n",
        "  ## original images\n",
        "  for i in range(n_samples):\n",
        "    plt.subplot(n_rows, n_cols, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_in[i])\n",
        "    \n",
        "  ## translated images in the second row\n",
        "  for i in range(n_samples):\n",
        "    plt.subplot(n_rows, n_cols, n_samples+i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_out[i])\n",
        "    \n",
        "  ## save the plot\n",
        "  filename = '%s_plot_e%02d_%04d.png' % (prefix, epoch, step+1)\n",
        "  path = os.path.join(directory, filename)\n",
        "  \n",
        "  plt.savefig(path)\n",
        "  plt.close()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apybkxn3LW3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define training\n",
        "def train(n_epochs, n_batch_size, sample_interval, dis_A, dis_B, gen_AB, gen_BA, comb_AB, comb_BA, trainA, trainB):\n",
        "  # get the patch size\n",
        "  n_patch = dis_A.output_shape[1]\n",
        "  \n",
        "  # number of batches per epoch\n",
        "  n_bat_per_epoch = int(len(trainA) / n_batch_size)\n",
        "  \n",
        "  # total number of steps to go thru\n",
        "  #n_steps = n_bat_per_epoch * n_epochs\n",
        "  \n",
        "  # get the start time\n",
        "  start_time = datetime.datetime.now()\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    #\n",
        "    # go thru each step for training\n",
        "    for i in range(n_bat_per_epoch):\n",
        "      # get the real samples from both domains\n",
        "      X_realA, y_realA = gen_real_samples(trainA, n_batch_size, n_patch)\n",
        "      X_realB, y_realB = gen_real_samples(trainB, n_batch_size, n_patch)\n",
        "\n",
        "      # get the translataed images for both domains\n",
        "      X_fakeA, y_fakeA = gen_fake_samples(gen_BA, X_realB, n_patch)\n",
        "      X_fakeB, y_fakeB = gen_fake_samples(gen_AB, X_realA, n_patch)\n",
        "\n",
        "      # original paper, updates the pool for fake images. TODO.\n",
        "\n",
        "      # Train generator_BA\n",
        "      gA_loss = comb_BA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
        "\n",
        "      #--------------------\n",
        "      # Train Discriminator A\n",
        "      #--------------------\n",
        "      dA_loss_real = dis_A.train_on_batch(X_realA, y_realA)\n",
        "      dA_loss_fake = dis_A.train_on_batch(X_fakeA, y_fakeA)\n",
        "\n",
        "      dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "      # Train generator_AB\n",
        "      gB_loss = comb_AB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
        "\n",
        "      #--------------------\n",
        "      # Train Discriminator B\n",
        "      #--------------------    \n",
        "      dB_loss_real = dis_B.train_on_batch(X_realB, y_realB)\n",
        "      dB_loss_fake = dis_B.train_on_batch(X_fakeB, y_fakeB)\n",
        "      dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "      # Total disciminator loss\n",
        "      d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "      # get the time\n",
        "      elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "      if ( i % sample_interval == 0):\n",
        "        print (\"[Epoch %d/%d] [Batch %d] [D loss: %f] [GA loss: %05f, GB loss: %05f ] time: %s \" \\\n",
        "                                                                        % ( epoch, n_epochs,\n",
        "                                                                            i,  \n",
        "                                                                            d_loss,\n",
        "                                                                            gA_loss[0],\n",
        "                                                                            gB_loss[0],                                                \n",
        "                                                                            elapsed_time))\n",
        "        \n",
        "        sample_images(epoch, i, gen_AB, trainA, 'AtoB', '/content/gen_images')\n",
        "        sample_images(epoch, i, gen_BA, trainB, 'BtoA', '/content/gen_images')\n",
        "        \n",
        "    # every epoch takes more than one hour is GPU\n",
        "    # so save models\n",
        "    #if ( epoch % 5 == 0):\n",
        "    save_models(epoch, '/content/drive/My Drive/App/CycleGAN/Faces/models', gen_AB, gen_BA, )\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPYYN283oiib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then normalize the data\n",
        "trainA = rescale_input(young_images)\n",
        "trainB = rescale_input(old_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaPIH4ai4GkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainA.shape\n",
        "#trainA[1]\n",
        "image_shape = trainA.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hc-JXWxR4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "a127629d-41aa-4d4c-c69f-ef4a3e5aa2ba"
      },
      "source": [
        "# define generators\n",
        "gen_AB = build_generator(image_shape)\n",
        "gen_BA = build_generator(image_shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 02:18:59.987335 139849894594432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0807 02:19:00.035341 139849894594432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0807 02:19:00.043441 139849894594432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emj2X5Cdzxpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d7bb83ff-ca47-450e-8b6e-71062706bd55"
      },
      "source": [
        "# define discriminators\n",
        "dis_A = build_discriminator(image_shape)\n",
        "dis_B = build_discriminator(image_shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 02:19:02.131406 139849894594432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES5-dQKz00L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the combined model\n",
        "combined_model_AB = build_combined_model(image_shape, gen_AB, dis_B, gen_BA)\n",
        "combined_model_BA = build_combined_model(image_shape, gen_BA, dis_A, gen_AB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHrD1u7yfb5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/gen_images\n",
        "!mkdir /content/models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdHlWzpoIcxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform training\n",
        "n_epochs = 10\n",
        "n_batch_size = 1\n",
        "#n_patch_size = dis_A.output_shape[1]\n",
        "sample_interval = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O95pqY4JhZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd3e1be8-edba-4f2f-9b65-ad85a6a905d1"
      },
      "source": [
        "train(n_epochs, n_batch_size, sample_interval,\n",
        "     dis_A, dis_B,\n",
        "     gen_AB, gen_BA,\n",
        "     combined_model_AB, combined_model_BA,\n",
        "     trainA, \n",
        "     trainB)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/25] [Batch 0] [D loss: 29.779428] [GA loss: 56.909424, GB loss: 43.933846 ] time: 0:00:04.822698 \n",
            "[Epoch 1/25] [Batch 200] [D loss: 0.221264] [GA loss: 7.319561, GB loss: 8.447161 ] time: 0:15:50.743221 \n",
            "[Epoch 1/25] [Batch 400] [D loss: 0.181200] [GA loss: 6.389479, GB loss: 4.979431 ] time: 0:31:36.329928 \n",
            "[Epoch 1/25] [Batch 600] [D loss: 0.077233] [GA loss: 5.844939, GB loss: 5.453511 ] time: 0:47:21.937351 \n",
            "[Epoch 1/25] [Batch 800] [D loss: 0.115275] [GA loss: 6.541596, GB loss: 7.084942 ] time: 1:03:12.232367 \n",
            "[Epoch 1/25] [Batch 1000] [D loss: 0.158471] [GA loss: 5.558513, GB loss: 6.144868 ] time: 1:19:01.240060 \n",
            "[Epoch 2/25] [Batch 0] [D loss: 0.163456] [GA loss: 4.463553, GB loss: 4.766647 ] time: 1:24:35.127520 \n",
            "[Epoch 2/25] [Batch 200] [D loss: 0.162577] [GA loss: 5.211803, GB loss: 4.741393 ] time: 1:40:23.742493 \n",
            "[Epoch 2/25] [Batch 400] [D loss: 0.083734] [GA loss: 3.419703, GB loss: 3.092314 ] time: 1:56:10.838608 \n",
            "[Epoch 2/25] [Batch 600] [D loss: 0.042899] [GA loss: 5.204477, GB loss: 5.365017 ] time: 2:11:55.747544 \n",
            "[Epoch 2/25] [Batch 800] [D loss: 0.086840] [GA loss: 3.943474, GB loss: 3.897169 ] time: 2:27:40.442437 \n",
            "[Epoch 2/25] [Batch 1000] [D loss: 0.108530] [GA loss: 5.649050, GB loss: 4.755801 ] time: 2:43:25.256038 \n",
            "[Epoch 3/25] [Batch 0] [D loss: 0.102648] [GA loss: 4.630559, GB loss: 4.526735 ] time: 2:47:52.129252 \n",
            "[Epoch 3/25] [Batch 200] [D loss: 0.156083] [GA loss: 3.249687, GB loss: 2.906170 ] time: 3:03:37.975537 \n",
            "[Epoch 3/25] [Batch 400] [D loss: 0.165046] [GA loss: 4.701806, GB loss: 4.285136 ] time: 3:19:27.731573 \n",
            "[Epoch 3/25] [Batch 600] [D loss: 0.084307] [GA loss: 4.168899, GB loss: 4.389531 ] time: 3:35:11.887484 \n",
            "[Epoch 3/25] [Batch 800] [D loss: 0.093512] [GA loss: 5.657704, GB loss: 5.325217 ] time: 3:51:04.792937 \n",
            "[Epoch 3/25] [Batch 1000] [D loss: 0.133670] [GA loss: 4.105472, GB loss: 4.005412 ] time: 4:07:03.621258 \n",
            "[Epoch 4/25] [Batch 0] [D loss: 0.125316] [GA loss: 6.463564, GB loss: 6.745143 ] time: 4:11:34.181460 \n",
            "[Epoch 4/25] [Batch 200] [D loss: 0.075854] [GA loss: 3.511993, GB loss: 3.504658 ] time: 4:27:31.907809 \n",
            "[Epoch 4/25] [Batch 400] [D loss: 0.111752] [GA loss: 3.450548, GB loss: 3.509571 ] time: 4:43:31.303713 \n",
            "[Epoch 4/25] [Batch 600] [D loss: 0.098820] [GA loss: 3.372544, GB loss: 2.445728 ] time: 4:59:29.170818 \n",
            "[Epoch 4/25] [Batch 800] [D loss: 0.109150] [GA loss: 4.646703, GB loss: 3.912627 ] time: 5:15:29.565240 \n",
            "[Epoch 4/25] [Batch 1000] [D loss: 0.111965] [GA loss: 5.221590, GB loss: 5.648137 ] time: 5:31:30.249982 \n",
            "[Epoch 5/25] [Batch 0] [D loss: 0.092074] [GA loss: 4.332200, GB loss: 3.757228 ] time: 5:36:03.265944 \n",
            "[Epoch 5/25] [Batch 200] [D loss: 0.048009] [GA loss: 3.453655, GB loss: 3.504386 ] time: 5:52:04.960306 \n",
            "[Epoch 5/25] [Batch 400] [D loss: 0.069926] [GA loss: 6.158434, GB loss: 6.138016 ] time: 6:08:05.910732 \n",
            "[Epoch 5/25] [Batch 600] [D loss: 0.174465] [GA loss: 4.155201, GB loss: 3.444060 ] time: 6:24:06.218690 \n",
            "[Epoch 5/25] [Batch 800] [D loss: 0.161567] [GA loss: 2.728415, GB loss: 2.954337 ] time: 6:40:06.076732 \n",
            "[Epoch 5/25] [Batch 1000] [D loss: 0.145748] [GA loss: 4.683811, GB loss: 5.294631 ] time: 6:56:06.825210 \n",
            "[Epoch 6/25] [Batch 0] [D loss: 0.092289] [GA loss: 2.911697, GB loss: 2.496471 ] time: 7:00:40.597933 \n",
            "[Epoch 6/25] [Batch 200] [D loss: 0.089525] [GA loss: 3.580670, GB loss: 2.831951 ] time: 7:16:49.120577 \n",
            "[Epoch 6/25] [Batch 400] [D loss: 0.119173] [GA loss: 2.085258, GB loss: 2.374787 ] time: 7:32:52.394395 \n",
            "[Epoch 6/25] [Batch 600] [D loss: 0.067777] [GA loss: 2.303066, GB loss: 2.539783 ] time: 7:48:59.360785 \n",
            "[Epoch 6/25] [Batch 800] [D loss: 0.121766] [GA loss: 2.717598, GB loss: 1.902062 ] time: 8:05:15.214085 \n",
            "[Epoch 6/25] [Batch 1000] [D loss: 0.119263] [GA loss: 4.194138, GB loss: 3.270845 ] time: 8:21:22.944959 \n",
            "[Epoch 7/25] [Batch 0] [D loss: 0.044265] [GA loss: 3.232490, GB loss: 3.281554 ] time: 8:25:54.308777 \n",
            "[Epoch 7/25] [Batch 200] [D loss: 0.066383] [GA loss: 2.626453, GB loss: 2.836332 ] time: 8:41:52.347496 \n",
            "[Epoch 7/25] [Batch 400] [D loss: 0.129120] [GA loss: 4.056925, GB loss: 3.586171 ] time: 8:57:50.758921 \n",
            "[Epoch 7/25] [Batch 600] [D loss: 0.147417] [GA loss: 2.401182, GB loss: 2.517883 ] time: 9:13:51.142454 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-689f8a2d5c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mcombined_model_AB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_model_BA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mtrainA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      trainB)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-51e14ab6f427>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, n_batch_size, sample_interval, dis_A, dis_B, gen_AB, gen_BA, comb_AB, comb_BA, trainA, trainB)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Train generator_BA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mgA_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomb_BA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;31m#--------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}